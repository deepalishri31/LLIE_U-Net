{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a7b20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:43:14.045322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 01:43:14.234165: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-23 01:43:14.234197: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-23 01:43:14.272718: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 01:43:14.936609: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 01:43:14.936698: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 01:43:14.936709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Training_Sony.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1GEBhrMgdTE81EYLi2hUD6Xrie9iRhLZg\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import os, time\n",
    "\n",
    "#pip install scipy==1.2.0\n",
    "\n",
    "import scipy as scipy\n",
    "\n",
    "# uniform content loss + adaptive threshold + per_class_input + recursive G\n",
    "# improvement upon cqf37\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive',force_remount = True)\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "#import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "#!pip install --upgrade tf_slim\n",
    "\n",
    "import tf_slim as slim\n",
    "\n",
    "import sys\n",
    "\n",
    "#pip install rawpy\n",
    "\n",
    "import rawpy as rawpy\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3b63c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "No. of Training Images: 161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dir = '/home/21i190003/DL_Project/Sony_Dataset/Sony/short/'\n",
    "gt_dir = '/home/21i190003/DL_Project/Sony_Dataset/Sony/long/'\n",
    "checkpoint_dir = '/home/21i190003/DL_Project/Sony_SSIM/Sony_ssim_ckpt/'\n",
    "result_dir = '/home/21i190003/DL_Project/Sony_SSIM/Sony_ssim_result/'\n",
    "print('------------------------------------------------------------------\\n------------------------------------------------------------------\\n------------------------------------------------------------------')        \n",
    "\n",
    "# get train IDs\n",
    "train_fns = glob.glob(gt_dir + '0*.ARW')\n",
    "train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n",
    "\n",
    "print(f'No. of Training Images: {len(train_ids)}') #Number of reference images for training for Sony Sensor\n",
    "\n",
    "ps = 512  # patch size for training\n",
    "save_freq = 5\n",
    "\n",
    "DEBUG = 0\n",
    "if DEBUG == 1:\n",
    "    save_freq = 1\n",
    "    train_ids = train_ids[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8256516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x):\n",
    "    return tf.maximum(x * 0.2, x)\n",
    "\n",
    "def upsample_and_concat(x1, x2, output_channels, in_channels):\n",
    "    pool_size = 2\n",
    "    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n",
    "    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n",
    "\n",
    "    deconv_output = tf.concat([deconv, x2], 3)\n",
    "    deconv_output.set_shape([None, None, None, output_channels * 2])\n",
    "\n",
    "    return deconv_output\n",
    "\n",
    "\n",
    "def pack_raw(raw):\n",
    "    # pack Bayer image to 4 channels\n",
    "    im = raw.raw_image_visible.astype(np.float32)\n",
    "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
    "\n",
    "    im = np.expand_dims(im, axis=2)\n",
    "    img_shape = im.shape\n",
    "    H = img_shape[0]\n",
    "    W = img_shape[1]\n",
    "\n",
    "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89180ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(input):\n",
    "    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n",
    "    #print(conv1.shape)\n",
    "    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n",
    "    #print(conv1.shape)\n",
    "    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n",
    "    #print(pool1.shape)\n",
    "    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n",
    "    #print(conv2.shape)\n",
    "    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n",
    "    #print(conv2.shape)\n",
    "    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n",
    "    #print(pool2.shape)    \n",
    "    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n",
    "    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n",
    "    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n",
    "\n",
    "    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n",
    "    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n",
    "    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n",
    "\n",
    "    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n",
    "    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n",
    "    #print(conv5.shape)\n",
    "    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n",
    "    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n",
    "    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n",
    "\n",
    "    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n",
    "    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n",
    "    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n",
    "\n",
    "    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n",
    "    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n",
    "    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n",
    "\n",
    "    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n",
    "    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n",
    "    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n",
    "\n",
    "    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n",
    "    #print(conv10.shape)\n",
    "    conv10 = slim.conv2d(conv10, 3, [2, 2], rate=1, activation_fn=None, scope='g_conv10_1')\n",
    "    out = conv10\n",
    "    print(input.shape)\n",
    "    print(out.shape)\n",
    "    #out = tf.depth_to_space(conv10, 2)\n",
    "    #print('Shape of U-Net output: ',out.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e38d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9357db49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Session....\n",
      "WARNING:tensorflow:From /home/21i190003/.local/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From /home/21i190003/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:44:22.097328: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-23 01:44:22.097392: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-23 01:44:22.097425: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kopri): /proc/driver/nvidia/version does not exist\n",
      "2022-11-23 01:44:22.097880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/21i190003/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, ?, 4)\n",
      "(?, ?, ?, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:44:23.815452: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "print('Initiating Session....')\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "tf.disable_eager_execution()\n",
    "in_image = tf.placeholder(tf.float32, [None, None, None, 4])\n",
    "gt_image = tf.placeholder(tf.float32, [None, None, None, 3])\n",
    "tf.disable_v2_behavior()\n",
    "out_image = network(in_image)\n",
    "#print(out_image.shape)\n",
    "#G_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n",
    "G_loss = tf.reduce_mean(tf.image.ssim(out_image, gt_image, 1.0))\n",
    "t_vars = tf.trainable_variables()\n",
    "lr = tf.placeholder(tf.float32)\n",
    "G_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "if ckpt:\n",
    "    print('loaded ' + ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "\n",
    "# Raw data takes long time to load. Keep them in memory after loaded.\n",
    "gt_images = [None] * 6000\n",
    "input_images = {}\n",
    "input_images['300'] = [None] * len(train_ids)\n",
    "input_images['250'] = [None] * len(train_ids)\n",
    "input_images['100'] = [None] * len(train_ids)\n",
    "\n",
    "g_loss = np.zeros((5000, 1))\n",
    "\n",
    "allfolders = glob.glob(result_dir + '*0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06a4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_freq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d94cd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "2 , Count:  1 , Train ID: 42 , Loss=-0.042 , Time=3.787\n",
      "2 , Count:  2 , Train ID: 24 , Loss=-0.060 , Time=3.695\n",
      "2 , Count:  3 , Train ID: 150 , Loss=-0.091 , Time=4.778\n",
      "2 , Count:  4 , Train ID: 44 , Loss=-0.085 , Time=4.709\n",
      "2 , Count:  5 , Train ID: 196 , Loss=-0.087 , Time=4.799\n",
      "2 , Count:  6 , Train ID: 97 , Loss=-0.096 , Time=4.742\n",
      "2 , Count:  7 , Train ID: 118 , Loss=-0.090 , Time=4.696\n",
      "2 , Count:  8 , Train ID: 85 , Loss=-0.087 , Time=4.736\n",
      "2 , Count:  9 , Train ID: 17 , Loss=-0.093 , Time=4.699\n",
      "2 , Count:  10 , Train ID: 181 , Loss=-0.092 , Time=4.713\n",
      "2 , Count:  11 , Train ID: 38 , Loss=-0.095 , Time=4.692\n",
      "2 , Count:  12 , Train ID: 138 , Loss=-0.096 , Time=4.730\n",
      "2 , Count:  13 , Train ID: 15 , Loss=-0.105 , Time=4.591\n",
      "2 , Count:  14 , Train ID: 123 , Loss=-0.118 , Time=4.624\n",
      "2 , Count:  15 , Train ID: 214 , Loss=-0.119 , Time=4.645\n",
      "2 , Count:  16 , Train ID: 175 , Loss=-0.122 , Time=4.655\n",
      "2 , Count:  17 , Train ID: 81 , Loss=-0.121 , Time=4.644\n",
      "2 , Count:  18 , Train ID: 62 , Loss=-0.126 , Time=4.636\n",
      "2 , Count:  19 , Train ID: 180 , Loss=-0.129 , Time=4.636\n",
      "2 , Count:  20 , Train ID: 67 , Loss=-0.134 , Time=4.594\n",
      "2 , Count:  21 , Train ID: 23 , Loss=-0.144 , Time=4.770\n",
      "2 , Count:  22 , Train ID: 59 , Loss=-0.156 , Time=4.647\n",
      "2 , Count:  23 , Train ID: 50 , Loss=-0.158 , Time=4.650\n",
      "2 , Count:  24 , Train ID: 232 , Loss=-0.162 , Time=4.677\n",
      "2 , Count:  25 , Train ID: 109 , Loss=-0.178 , Time=4.707\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             gt_patch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(gt_patch, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     62\u001b[0m input_patch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mminimum(input_patch, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m _, G_current, output \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mG_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_image\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43min_image\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_patch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_image\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_patch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mminimum(np\u001b[38;5;241m.\u001b[39mmaximum(output, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m g_loss[ind] \u001b[38;5;241m=\u001b[39m G_current\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training Started------------------------------------------------------------------\\n------------------------------------------------------------------\\n------------------------------------------------------------------')        \n",
    "np.random.seed(10)\n",
    "lastepoch = 0\n",
    "for folder in allfolders:\n",
    "    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for epoch in range(lastepoch, 3):\n",
    "    if os.path.isdir(result_dir + '%04d' % epoch):\n",
    "        continue\n",
    "    cnt = 0\n",
    "    if epoch > 2000:\n",
    "        learning_rate = 1e-5\n",
    "\n",
    "    for ind in np.random.permutation(len(train_ids)):\n",
    "        # get the path from image id\n",
    "        train_id = train_ids[ind]\n",
    "        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n",
    "        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n",
    "        in_fn = os.path.basename(in_path)\n",
    "\n",
    "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n",
    "        gt_path = gt_files[0]\n",
    "        gt_fn = os.path.basename(gt_path)\n",
    "        in_exposure = float(in_fn[9:-5])\n",
    "        gt_exposure = float(gt_fn[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "\n",
    "        st = time.time()\n",
    "        cnt += 1\n",
    "\n",
    "        if input_images[str(ratio)[0:3]][ind] is None:\n",
    "            #try:\n",
    "                raw = rawpy.imread(in_path)\n",
    "                input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
    "\n",
    "                gt_raw = rawpy.imread(gt_path)\n",
    "                im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "                gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
    "        # crop\n",
    "        H = input_images[str(ratio)[0:3]][ind].shape[1]\n",
    "        W = input_images[str(ratio)[0:3]][ind].shape[2]\n",
    "\n",
    "        xx = np.random.randint(0, W - ps)\n",
    "        yy = np.random.randint(0, H - ps)\n",
    "        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n",
    "        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
    "\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
    "            input_patch = np.flip(input_patch, axis=1)\n",
    "            gt_patch = np.flip(gt_patch, axis=1)\n",
    "        if np.random.randint(2, size=1)[0] == 1:\n",
    "                    input_patch = np.flip(input_patch, axis=2)\n",
    "                    gt_patch = np.flip(gt_patch, axis=2)\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
    "                    input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
    "                    gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n",
    "\n",
    "        input_patch = np.minimum(input_patch, 1.0)\n",
    "\n",
    "        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n",
    "                                                feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n",
    "        output = np.minimum(np.maximum(output, 0), 1)\n",
    "        g_loss[ind] = G_current\n",
    "        #print(g_loss[np.where(g_loss)])\n",
    "        print(\"%d , Count:  %d , Train ID: %d , Loss=%.3f , Time=%.3f\" % ((epoch+1), cnt,train_id, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n",
    "\n",
    "        if epoch % save_freq == 0:\n",
    "            if not os.path.isdir(result_dir + '%04d' % epoch):\n",
    "                os.makedirs(result_dir + '%04d' % epoch)\n",
    "\n",
    "            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n",
    "            Image.fromarray((temp * 255).astype(np.uint8),mode='RGB').save(\n",
    "                        result_dir + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))        \n",
    "        #except:\n",
    "        #    print(f\"Oops! {sys.exc_info()[0]} occurred for Train ID {train_id}, moving to next training ID....\")\n",
    "\n",
    "        #if cnt>=100:\n",
    "        #    break\n",
    "    print(f'------------------------------------------------------------------\\nEpoch: {epoch+1} to Epoch: {epoch+2}------------------------------------------------------------------\\n------------------------------------------------------------------')        \n",
    "    if not os.path.isdir(result_dir + 'model.ckpt'):\n",
    "      os.makedirs(result_dir + 'model.ckpt')\n",
    "    saver.save(sess, checkpoint_dir + 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c20a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
